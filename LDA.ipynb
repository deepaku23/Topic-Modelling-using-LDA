{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file                                            message\n",
       "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
       "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
       "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
       "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
       "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirichlet , hyperparameters , tokenizer\n",
    "\n",
    "ALPHA = 0.1 \n",
    "BETA = 0.1\n",
    "NUM_TOPICS = 20 # we can use coherence score it measures how similar are the words inside a topic and based on that we can change the number of topics \n",
    "\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get to know how many time a word occurs \n",
    "def generate_frequencies(data, max_docs = 10000): \n",
    "    freqs = Counter()\n",
    "    all_stopwords = sp.Defaults.stop_words\n",
    "    all_stopwords.add(\"enron\")\n",
    "    nr_tokens = 0\n",
    "    \n",
    "    for doc in data[:max_docs]:\n",
    "        tokens = sp.tokenizer(doc)\n",
    "        for token in tokens:\n",
    "            token_text = token.text.lower()\n",
    "            #(contains alphabets and no number)\n",
    "            if token_text not in all_stopwords and token.is_alpha: #(contains alphabets and no number)\n",
    "                nr_tokens += 1\n",
    "                freqs[token_text] += 1\n",
    "    return freqs\n",
    "\n",
    "\n",
    "\n",
    "def get_vocab(freqs, freq_thresholds = 3):\n",
    "    vocab = {}\n",
    "    vocab_idx_str = {}\n",
    "    vocab_idx = 0\n",
    "\n",
    "    for word in freqs:\n",
    "        if freqs[word] >= freq_thresholds:\n",
    "            vocab[word] = vocab_idx\n",
    "            vocab_idx_str[vocab_idx] = word\n",
    "            vocab_idx += 1\n",
    "    return vocab, vocab_idx_str\n",
    "\n",
    "\n",
    "def tokenize_dataset(data, vocab, max_docs = 10000):\n",
    "    nr_tokens = 0\n",
    "    nr_docs = 0\n",
    "    docs =[]\n",
    "\n",
    "    for doc in data[:max_docs]:\n",
    "        tokens = sp.tokenizer(doc)\n",
    "\n",
    "        if len(tokens) > 1:\n",
    "            doc = []\n",
    "            for token in tokens:\n",
    "                token_text = token.text.lower()\n",
    "                if token_text in vocab:\n",
    "                    doc.append(token_text)\n",
    "                    nr_tokens +=1\n",
    "            nr_docs +=1\n",
    "            docs.append(doc)\n",
    "    print(f\"Number of documents : {nr_docs}\")\n",
    "    print(f\"Number of tokens : {nr_tokens}\")\n",
    "\n",
    "    # Numericalise\n",
    "\n",
    "    corpus = []\n",
    "    for doc in docs:\n",
    "        corpus_d = []\n",
    "\n",
    "        for token in doc:\n",
    "            corpus_d.append(vocab[token])\n",
    "\n",
    "        corpus.append(np.asarray(corpus_d))\n",
    "    \n",
    "    return docs, corpus \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['message'].sample(frac = 0.001, random_state = 42).values # take 50% of the data, and convert that to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents : 517\n",
      "Number of tokens : 88538\n",
      "Vocab size: 4966\n"
     ]
    }
   ],
   "source": [
    "freqs = generate_frequencies(data)\n",
    "vocab, vocab_idx_str = get_vocab(freqs)\n",
    "docs, corpus = tokenize_dataset(data, vocab)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 4966\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [05:14<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "def LDA_Collapsed_Gibbs(corpus, max_iter = 200):\n",
    "\n",
    "    # PART1: initialise counts and Z(topcis)\n",
    "    Z =[]\n",
    "    num_docs = len(corpus)\n",
    "    for _, doc in enumerate(corpus):\n",
    "        # for each documnet in the corpus\n",
    "        Zd = np.random.randint(low =0, high = NUM_TOPICS, size = len(doc)) # assign a topic to each word at random\n",
    "        Z.append(Zd)\n",
    "    \n",
    "    # ndk -for a particular document how are the topcis disributed\n",
    "    ndk = np.zeros((num_docs,NUM_TOPICS))\n",
    "    for d in range(num_docs):\n",
    "        for k in range(NUM_TOPICS):\n",
    "            ndk[d,k] = np.sum(Z[d] == k)\n",
    "\n",
    "    # nwk - the topics and its words\n",
    "    nkw = np.zeros((NUM_TOPICS, vocab_size))\n",
    "    for doc_idx, doc in enumerate(corpus):\n",
    "        for i, word in enumerate(doc):\n",
    "            topic = Z[doc_idx][i]\n",
    "            nkw[topic, word] +=1\n",
    "\n",
    "\n",
    "    nk = np.sum(nkw, axis = 1) # how many words are there in each topics \n",
    "    topic_list = [i for i in range(NUM_TOPICS)]\n",
    "\n",
    "    # PART2: loop\n",
    "\n",
    "    for _ in tqdm(range(max_iter)):\n",
    "        for doc_idx, doc in enumerate(corpus):\n",
    "            for i in range(len(doc)): # for all the words in that document \n",
    "                word = doc[i]\n",
    "                topic = Z[doc_idx][i]\n",
    "\n",
    "                # remove z_i because conditioned on z_(-i)\n",
    "                ndk[doc_idx, topic] -= 1\n",
    "                nkw[topic, word] -= 1\n",
    "                nk[topic] -= 1\n",
    "\n",
    "            # prob of a word belonging to a certain topic given others\n",
    "                p_z = (ndk[doc_idx, :] + ALPHA) * (nkw[:, word] + BETA) / (nk[:] + BETA * vocab_size) \n",
    "                topic = random.choices(topic_list, weights = p_z, k = 1)[0]\n",
    "\n",
    "                # update nn parameters \n",
    "                Z[doc_idx][i] = topic\n",
    "                ndk[doc_idx, topic] += 1\n",
    "                nkw[topic, word] += 1\n",
    "                nk[topic] += 1\n",
    "\n",
    "    return Z, ndk, nkw, nk\n",
    "\n",
    "Z, ndk, nkw, nk = LDA_Collapsed_Gibbs(corpus)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 most common words\n",
      "program\n",
      "going\n",
      "day\n",
      "time\n",
      "way\n",
      "school\n",
      "good\n",
      "florida\n",
      "national\n",
      "people\n",
      "\n",
      "\n",
      "Topic 1 most common words\n",
      "cost\n",
      "energy\n",
      "month\n",
      "customers\n",
      "price\n",
      "customer\n",
      "rate\n",
      "renewable\n",
      "database\n",
      "generation\n",
      "\n",
      "\n",
      "Topic 2 most common words\n",
      "subject\n",
      "sent\n",
      "intended\n",
      "e\n",
      "recipient\n",
      "mail\n",
      "use\n",
      "data\n",
      "information\n",
      "confidential\n",
      "\n",
      "\n",
      "Topic 3 most common words\n",
      "houston\n",
      "pm\n",
      "beck\n",
      "december\n",
      "tuesday\n",
      "vince\n",
      "sally\n",
      "tickets\n",
      "available\n",
      "chris\n",
      "\n",
      "\n",
      "Topic 4 most common words\n",
      "jeff\n",
      "dasovich\n",
      "james\n",
      "richard\n",
      "california\n",
      "steffes\n",
      "e\n",
      "ferc\n",
      "market\n",
      "mail\n",
      "\n",
      "\n",
      "Topic 5 most common words\n",
      "said\n",
      "riordan\n",
      "section\n",
      "business\n",
      "freeman\n",
      "working\n",
      "meeting\n",
      "meetings\n",
      "nemec\n",
      "staff\n",
      "\n",
      "\n",
      "Topic 6 most common words\n",
      "iso\n",
      "ferc\n",
      "market\n",
      "data\n",
      "ufe\n",
      "settlement\n",
      "fee\n",
      "report\n",
      "project\n",
      "december\n",
      "\n",
      "\n",
      "Topic 7 most common words\n",
      "x\n",
      "cc\n",
      "agreement\n",
      "content\n",
      "subject\n",
      "attached\n",
      "jones\n",
      "tana\n",
      "bcc\n",
      "charset\n",
      "\n",
      "\n",
      "Topic 8 most common words\n",
      "x\n",
      "content\n",
      "subject\n",
      "cc\n",
      "bcc\n",
      "date\n",
      "message\n",
      "version\n",
      "origin\n",
      "mime\n",
      "\n",
      "\n",
      "Topic 9 most common words\n",
      "said\n",
      "company\n",
      "credit\n",
      "financial\n",
      "billion\n",
      "gas\n",
      "rating\n",
      "markets\n",
      "dow\n",
      "million\n",
      "\n",
      "\n",
      "Topic 10 most common words\n",
      "image\n",
      "communications\n",
      "click\n",
      "email\n",
      "e\n",
      "address\n",
      "free\n",
      "price\n",
      "web\n",
      "internet\n",
      "\n",
      "\n",
      "Topic 11 most common words\n",
      "cn\n",
      "recipients\n",
      "ou\n",
      "na\n",
      "non\n",
      "notesaddr\n",
      "items\n",
      "pjm\n",
      "mark\n",
      "williams\n",
      "\n",
      "\n",
      "Topic 12 most common words\n",
      "x\n",
      "content\n",
      "cc\n",
      "date\n",
      "subject\n",
      "bcc\n",
      "message\n",
      "version\n",
      "id\n",
      "folder\n",
      "\n",
      "\n",
      "Topic 13 most common words\n",
      "ohio\n",
      "court\n",
      "action\n",
      "state\n",
      "image\n",
      "law\n",
      "click\n",
      "shall\n",
      "steel\n",
      "public\n",
      "\n",
      "\n",
      "Topic 14 most common words\n",
      "hou\n",
      "pm\n",
      "ect\n",
      "corp\n",
      "forwarded\n",
      "cc\n",
      "subject\n",
      "na\n",
      "sara\n",
      "shackleton\n",
      "\n",
      "\n",
      "Topic 15 most common words\n",
      "transmission\n",
      "power\n",
      "edison\n",
      "service\n",
      "aep\n",
      "testimony\n",
      "cities\n",
      "energy\n",
      "provide\n",
      "mw\n",
      "\n",
      "\n",
      "Topic 16 most common words\n",
      "hou\n",
      "david\n",
      "delainey\n",
      "committee\n",
      "plan\n",
      "kean\n",
      "dave\n",
      "w\n",
      "ken\n",
      "employees\n",
      "\n",
      "\n",
      "Topic 17 most common words\n",
      "energy\n",
      "power\n",
      "california\n",
      "gas\n",
      "market\n",
      "prices\n",
      "electricity\n",
      "new\n",
      "natural\n",
      "crisis\n",
      "\n",
      "\n",
      "Topic 18 most common words\n",
      "scott\n",
      "germany\n",
      "gas\n",
      "chris\n",
      "contract\n",
      "hou\n",
      "deal\n",
      "farmer\n",
      "tw\n",
      "available\n",
      "\n",
      "\n",
      "Topic 19 most common words\n",
      "e\n",
      "mail\n",
      "john\n",
      "m\n",
      "mark\n",
      "l\n",
      "lisa\n",
      "scott\n",
      "mike\n",
      "joe\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phi = nkw / np.reshape(NUM_TOPICS, 1) # to get probability distribution \n",
    "\n",
    "num_words = 10\n",
    "for k in range(NUM_TOPICS):\n",
    "    most_common_words = np.argsort(phi[k])[::-1][:num_words]\n",
    "    print(f\"Topic {k} most common words\")\n",
    "\n",
    "    for word in most_common_words:\n",
    "        print(vocab_idx_str[word])\n",
    "\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
